% !TeX root = bachelorarbeit.tex
Nachdem wir in Abschnitt 2 an einige Grundlagen erinnert, in Abschnitt 3 und 4 sowohl die Monte Carlo Methode, als auch die Multilevel Monte Carlo Methode eingeführt und in Abschnitt 5 das Transportproblem, sowie das Potentialströmungsproblem inklusive numerischer Verfahren, welche zur Lösung dergleichen genutzt werden können, erklärt haben, soll nun dieser Abschnitt dazu dienen, die bisherigen Ergebnisse zu bündeln und die Anwendung der Multilevel Monte Carlo Methode auf partielle Differentialgleichungen am Beispiel des Transportproblem nahe zu legen. Dabei nimmt dieser Abschnitt einen zentralen Platz in dieser Thesis ein, weswegen wir noch einmal darlegen wollen, was genau unser Ziel ist und wie wir die uns an dieser Stelle zu Verfügung stehenden Mittel einsetzen, um das Gewünschte zu erreichen.
\begin{align*}
&\text{Für ein stochastisches Flussvektorfeld } q: \Omega \times \overline{\mathbb{D}} \to \R^2 \text{, bestimme }\rho: \Omega \times \overline{\mathbb{D}} \times \mathbb{T} \to \R_{\geq 0} \text{ mit} \\
&\text{(pTP)} 
\begin{cases}
\begin{array}{rlll}
\partial_t \rho (\omega, x, t) + \dive(\rho(\omega,x,t)q(\omega,x)) &= 0 &\text{, für } (x,t) \in \mathbb{D} \times (0,T] \\
\rho(\omega,x,t) &= \rho_{\text{in}}(x,t) &\text{, für } (x,t) \in \Gamma_{\text{in}} \times \mathbb{T} \\
\rho(\omega,x,0)  &= \rho_0(x) &\text{, für } x \in  \mathbb{D}
\end{array}
\end{cases} \\
&\text{ für die Anfangs- und Randwerte: } \\ 
&\begin{array}{llr}
g_N&: \Gamma_{\text{N}} \to \R \\
u_D&: \Gamma_{\text{D}} \to \R \\
\rho_{\text{in}}&: \Gamma_{\text{in}} \times \mathbb{T} \to \R_{\geq0} \\
\rho_0&: \mathbb{D} \to \R_{\geq0} \\
\end{array} \newline \\
&\text{ wobei } \partial \mathbb{D} = \Gamma_{\text{D}} \dot{\cup} \Gamma_{\text{N}}  \text{ und }  \Gamma_{\text{in}} \coloneqq  \{ z \in \partial \mathbb{D}: q(z)\cdot n(z) \leq 0 \} \subset  \partial \mathbb{D}
\end{align*}
Genauer sei $ Q(\omega) = J(\rho(\omega)) $ ein gegebenes Zielfunktional, dann ist es unser Ziel den Erwartungswert $ \mathbb{E}[Q(\omega)] $ möglichst genau zu bestimmen. Beispielsweise kann eine beliebige Norm von $ \rho(\omega) $ als Zielfunktional betrachtet werden.
Unser Modellproblem lautet also:

\begin{align}
\text{(MP)}
\begin{cases}
\label{Modellproblem}
&\text{Für ein stochastisches Flussvektorfeld } q: \Omega \times \overline{\mathbb{D}} \to \R^2 \\
&\text{und ein Zielfunktional } J \text{ , bestimme }  \mathbb{E}[J(\rho)]  \\
&\text{mit } \rho \text{ als Lösung von (pTP) inklusive Anfangs- und Randwerten}
\end{cases}
\end{align} 
Dabei erhalten wir das stochastische Flussvektorfeld $ q $, wie bereits in \ref{Probabilistisches Problem} erklärt selbst als Lösung des Potentialströmungsproblem.
Da wir an der numerischen Lösung partieller Differentialgleichungen interessiert sind und wir die im Allgemeinen unendlich dimensionale Lösung $ \rho $ durch eine endlich dimensionale Lösung $ \rho_{h,\Delta t} $ approximieren, betrachten wir $ Q_{h,\Delta t}(\omega) \coloneqq J(\rho_{h,\Delta t}(\omega )) $.
Wir nutzen dabei das in Abschnitt \ref{DG} behandelte Discontinuous Galerkin Verfahren zur Lösung des linearen Transportproblems. Außerdem werden wir im Folgenden eine uniforme Familie $ \{ \mathcal{T}_h \} $ von Zerlegungen von $ \mathcal{D} $ (vgl. Abschnitt \ref{num_pot} Definition \ref{FEMDISC}) als Diskretisierungsgitter für die Ortsdiskretisierung, sowie $ \mathbb{T}_{\Delta t} $ als Zerlegung von $ \mathbb{T} $ betrachten.
Insbesondere wählen wir später $ \Delta t $ in Abhängigkeit von $ h $, etwa $ \Delta t = c  h $ für ein $ c>0 $ und betrachten dann
$ Q_h(\omega) \coloneqq Q_{h,ch}(\omega) = J(\rho_h(\omega )) \coloneqq J(\rho_{h,ch}(\omega )) $.
An dieser Stelle sei betont, dass es sich in diesem Abschnitt bei $ \rho_h $ um eine volldiskrtisierte Approximation an $ \rho $ handelt und nicht die Semidiskretisierung aus Abschnitt \ref{DG} gemeint ist. Um die Notation im Folgenden zu erleichtern schreiben wir für $ a,b \in \R_{> 0} $ $ a \lesssim b $, falls $\frac{a}{b}$ gleichmäßig beschränkt und insbesondere unabhängig von den Parametern $ h $ und $ n $ ist.
\begin{Annahme}(Konvergenz im Erwartungswert des Zielfunktionals)\\
	\label{Annahme1}
	In obiger Situation gelte 
	\[ 
	\mathbb{E}[Q_h] \to \mathbb{E}[Q] \text{ für } h \to 0   
	\]
	Genauer existiere ein $ \alpha > 0 $, sodass
	\[
		\abs{\mathbb{E}[Q_h-Q]} \lesssim h^{\alpha}
	\]
	Wir nennen $ \alpha $ dann auch die Konvergenzrate von $ Q_h $.
\end{Annahme}


\subsection{Die Monte Carlo Methode}
Wie bereits im Kontext der numerischen Integration wollen wir die Monte Carlo Methode als Ausgangspunkt nutzen und anschließend bei der Betrachtung der Multilevel Monte Carlo Methode auch auf entscheidende Unterschiede zu und Vorteile gegenüber der Monte Carlo Methode eingehen.
Sowohl bei der Monte Carlo Methode, als auch bei der Multilevel Variante approximieren wir den Erwartungswert $ \mathbb{E}[Q_h] $ durch einen Schätzwert $ \widehat{Q}_h $. Um die Genauigkeit und die Kosten zu bemessen betrachten wir zum einen den sogenannten 'root mean square error' (RMSE) 
\begin{align}
	\label{RMSE}
	e( \widehat{Q}_h) \coloneqq \left(  \mathbb{E} \left[ (\widehat{Q}_h - \mathbb{E}[Q] )^2 \right] \right)^{\frac{1}{2}}
\end{align}
zum anderen die Anzahl an floating-point-Rechenoperationen $ C_{\epsilon}(\widehat{Q}_h) $, die benötigt werden um einen RMSE mit $ e( \widehat{Q}_h) \leq \epsilon $ zu erhalten.
Zu beachten ist hierbei, dass in dem RMSE einige Fehlerquellen gemeinsam betrachtet werden. So gehen sowohl der Finite Elemente Fehler des Potentialströmungsproblems (Approximation von $ q $ durch $ q_h $), der Discontinuous Galerkin Fehler des Transportproblems (Approximation von $ \rho $ durch $ \rho_h $), der Approximationsfehler der Approximation von $ Q $ durch $ Q_h $, als auch der statistische Fehler des Schätzers $\widehat{Q}_h$ in den RMSE mit ein. Insbesondere bedeutet also $ e( \widehat{Q}_h) \leq \epsilon $, dass alle oben genannten Fehlerquellen kleiner als $ \epsilon $ ausfallen. Betrachten wir also, wie bei der Monte Carlo Methode, welche wir gleich noch einmal kurz behandeln wollen, nur eine einzige Zerlegung $ \mathcal{T} $ der Familie $ \mathcal{T}_h $, so kann es sein, dass es gar nicht möglich ist den RMSE durch ein bestimmtes $ \epsilon $ zu beschränken, da einer der Approximationsfehler für das gewählte feste Level bereits alleine größer als $ \epsilon $ ist. Wir wollen diesen Sachverhalt im Hinterkopf behalten, denn auch bei der Multilevel Monte Carlo  Methode werden wir später initiale Level wählen, in der konkreten Anwendung geben wir aus praktischen Gründen sogar ein maximales Level an, welches wiederum unter Umständen einen RMSE $ e( \widehat{Q}_h) \leq \epsilon $ verhindern kann.\\
Bei der Standard Monte Carlo Methode schätzen wir $ \mathbb{E}[Q] $ durch den Mittelwert  $ n $ unabhängiger gleichverteilter Zufallssamples und erhalten so 
\begin{align}
	\label{MC-Schätzer}
	\widehat{Q}_{h,n}^{\text{MC}} \coloneqq \frac{1}{n} \sum_{i=1}^{n} Q_h(\omega_i) = \sum_{i=1}^{n} J(\rho_h(\omega_i))
\end{align}
Dabei modellieren wir das zufällige Flussvektorfeld $ q(\omega_i)  : \mathbb{D} \to \R^2 $, indem wir zunächst für $ \kappa(\omega_i) : \mathbb{D} \rightarrow (\R_{\text{sym}})^{d \times d} $ ein lognormal-verteiltes unabhängiges Zufallsfeld erzeugen und anschließend das Potentialströmungsproblem
\begin{align*}
\setlength\arraycolsep{1pt}
&\text{Für } \kappa(\omega_i) : \mathbb{D} \rightarrow (\R_{\text{sym}})^{d \times d} \text{, bestimme } u(\omega_i,\cdot):\overline{\mathbb{D}} \to \R \text{ und } q(\omega_i, \cdot): \overline{\mathbb{D}} \to \R^2 \text{ mit } \\
&\text{(PS)}
\begin{cases}
\begin{array}{rlll}
\dive (q(\omega_i,x)) &= 0  &\text{, für } x \in \mathbb{ D}\\  
q(\omega_i,x) &= - \kappa(\omega_i) \nabla u(\omega_i,x)  &\text{, für } x \in \mathbb{D}\\
-q(\omega_i,x) \cdot n &= g_N(x)  &\text{, für } x \in \Gamma_N \\
u(\omega_i,x) &= u_D(x)  &\text{, für } x \in \Gamma_D \\
\end{array}
\end{cases} \\
\end{align*}
lösen. Zur Erzeugung des lognormal-verteilten Zufallsfeldes können wir auf entsprechende Algorithmen zurückgreifen, in unserem Fall etwa dem sogenannten Circulant Embedding.
Der Algorithmus wurde 1997 erstmals in \cite{dietrich1997fast} vorgestellt und 
erzeugt Gauß'sche Zufallsfelder auf regulären Gittern und basiert auf der Fast Fourier Transformation, welche in der Literatur auch oft unter der Abkürzung FFT zu finden ist.
Dabei werden spezielle Strukturen der Kovarianzmatrix ausgenutzt. Anschließend kann das Gauß'sche Zufallsfeld über eine einfache Transformation in ein lognormal Feld überführt werden. Mehr zu Circulant Embedding findet sich z.B. in \cite{schmidt2014stochastic} Abschnitt 12.
Auch die grundsätzliche Idee Circulant Embedding für die Modellierung der Ausgangsdaten in stochastischen partiellen Differentialgleichungen zu nutzen ist keineswegs neu und findet sich z.B. in \cite{charrier2012strong} oder \cite{cliffe2011multilevel}.
Wir haben nun alle Mittel in der Hand um die Monte Carlo Methode angewandt auf das Transportproblem als Algorithmus zu formulieren. Dabei fassen wir die eben erklärte Erzeugung des zufälligen Vektorfeldes in der Funktion 'RndVecField' zusammen.
Außerdem gehen wir davon aus, dass bereits alle übrigen freien Parameter, sowie die Rand- und Anfangswerte fest gewählt sind.
\begin{algorithm}[H]
	\DontPrintSemicolon
	\SetAlgoLined
	%\KwResult{}
	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
	\Input{$h,n$}
	\Output{$\widehat{Q}_{h,n}^{\text{MC}}$}
	\BlankLine
	Initialisiere: $ \Sigma =0, i=0 $\;
	\While{$i<n$}{
		Erzeuge ein Zufallssample: $ q(\omega_i,x)  \leftarrow $ RndVecField\;
		Löse das Transportproblem: $ \rho_{h}(\omega_i,x)  \leftarrow$ Löse mit Discontinuous Galerkin\;
		Berechne das zugehörige Zielfuinktional: $ Q_h(\omega_i) \leftarrow$ Berechne Zielfunktional\;
		Setze: $ \Sigma = \Sigma + Q_h(\omega_i) $, i = i+1\;
	}
	\BlankLine
	\KwResult{$\widehat{Q}_{h,n}^{\text{MC}} = \Sigma /n$}
	\caption{Monte Carlo Methode angewandt auf das Transportproblem}
\end{algorithm}\bigskip % add 12pt space in-between

Wir nehmen an dieser Stelle an, dass sich die Gesamtkosten an Rechenoperationen, welche für die Berechnung eines $ Q_h(\omega_i) $ benötigt werden, durch
 \[
 	C(Q_h(\omega_i))  \lesssim h^{- \gamma}
 \] beschränken lassen.
 Im Folgenden wollen uns überlegen, wie wir unter dieser Annahme die Kosten für $ C_{\epsilon} $ abschätzen können. Diese Überlegungen finden sich auch in \cite{cliffe2011multilevel}.
 Der 'mean square error' $ e(\widehat{Q}_{h,n}^{\text{MC}})^2 $, also das Quadrat des RMSE, lässt sich nämlich auch folgendermaßen betrachten:
 \begin{align}
 \label{badumts}
 	e(\widehat{Q}_{h,n}^{\text{MC}})^2 &= \mathbb{E} \left[ \left( \widehat{Q}_{h,n}^{\text{MC}} -  \mathbb{E}[\widehat{Q}_{h,n}^{\text{MC}}] + \mathbb{E}[\widehat{Q}_{h,n}^{\text{MC}}] - \mathbb{E}[Q] \right)^2 \right] \nonumber \\
 	&= \mathbb{E} \left[ ( \widehat{Q}_{h,n}^{\text{MC}} -    \mathbb{E}[\widehat{Q}_{h,n}^{\text{MC}}]])^2 \right] + \left( \mathbb{E}[\widehat{Q}_{h,n}^{\text{MC}}] - \mathbb{E}[Q] \right)^2 \nonumber \\
 	&= \mathbb{V}[\widehat{Q}_{h,n}^{\text{MC}}] + \left( \mathbb{E}[\widehat{Q}_{h,n}^{\text{MC}}] - \mathbb{E}[Q] \right)^2
 \end{align}
 Da weiter $ \mathbb{E}[\widehat{Q}_{h,n}^{\text{MC}}] = \mathbb{E}[Q_h] $ und $ \mathbb{V}[\widehat{Q}_{h,n}^{\text{MC}}] = \frac{1}{n^2} n \mathbb{V}[Q_h] = \frac{1}{n}  \mathbb{V}[Q_h] $ gilt, erhalten wir so 
 \begin{align}
 \label{mseMC}
	 e(\widehat{Q}_{h,n}^{\text{MC}})^2 = \underbrace{\frac{1}{n}  \mathbb{V}[Q_h]}_{(1)} + \underbrace{(\mathbb{E}[Q_h - Q])^2}_{(2)} \ .
 \end{align}
 Dabei ist (1), wie wir eben bereits gesehen haben, die Varianz des Monte Carlo Schätzers und spiegelt daher den Schätzfehler wider. Unter der zusätzlichen Annahme, dass Erwartungswert und Varianz des Schätzers existieren, konvergiert dieser Fehler nach dem starken Gesetz großer Zahlen für $ n \to \infty $ gegen $ 0 $.
 (2) hingegen hängt als Quadrat über den Gesamtapproximationsfehler zwischen $ Q_h $
 und $ Q $ nur von der Diskretisierungsschrittweite $ h $ ab. Ist das verwendete Lösungsverfahren konvergent, so geht (2) gegen $ 0 $ für $ h \to 0 $.
 Wollen wir den RMSE also durch ein $ \epsilon > 0 $ beschränken, ist es hinreichend dafür zu sorgen, dass sowohl (1) also auch (2) kleiner als $ \frac{\epsilon^2}{2} $ ausfallen. Wir müssen also $ h $ klein genug und zugleich $ n $ groß genug wählen.
 Genauer kann dies, unter der Annahme, dass $ \mathbb{V}[Q_h] $ annäherend konstant ist und somit nicht von $ h $ abhängt, erreicht werden, indem wir $ n \gtrsim \epsilon^{-2} $ und $ h \lesssim \epsilon^{\frac{1}{\alpha}}  $ (vgl. Annahme \ref{Annahme1}).
 Da wir weiter angenommen hatten, dass $ C(Q_h(\omega_i))  \lesssim h^{- \gamma} $ gilt, erhalten wir 
 \[ 
 	C(\widehat{Q}_{h,n}^{\text{MC}}) \lesssim n h^{-\gamma}
 \]
 und somit Gesamtkosten für einen erwarteten Schätzfehler kleiner als $ \epsilon $ von
 \begin{align}
	 C_{\epsilon}(\widehat{Q}_{h,n}^{\text{MC}}) \lesssim \epsilon^{-2-\frac{\gamma}{\alpha}}
 \end{align}
 
\subsection{Die Multilevel Monte Carlo Methode}

Wie wir bereits in Abschnitt \ref{MLMC} dargelegt haben, ist die entscheidende Idee der Multilevel Monte Carlo Methode Zufallssamples auf mehreren verschiedenen Leveln $ \{ h_l : l = 0,\dots,L \} $ mit $ h_0 > h_1 > \cdots > h_L $ zu betrachten. Wir erinnern an dieser Stelle daran, dass wir für die Menge aller betrachtbaren Level $ \mathcal{H} = \{ h_0 , h_1 \coloneqq \frac{h_0}{2},h_2 \coloneqq \frac{h_1}{2} = \frac{h_0}{4}, \dots \} $ gefordert hatten, insbesondere war hier $0 \in \overline{\mathcal{H}}$. Theoretisch gesehen berechnet der Algorithmus, welchen wir an späterer Stelle noch formulieren wollen, für ein gegebenes $ \epsilon $ nur Samples auf den für diese Genauigkeit benötigten Leveln, einer endlichen Untermenge von $ \mathcal{H} $. Die Menge der tatsächlich betrachteten Samples $ \tilde{\mathcal{H}} = \{ h_l : l = 0,\dots,L \} \subset \mathcal{H} $ ist also stets endlich.
Wie bereits in \ref{MLMC} erhalten wir 
\[
\mathbb{E}[Q_{h_L}] = \mathbb{E}[Q_{h_0}] + \sum_ {l=0}^L \mathbb{E}[Q_{h_l}-Q_{h_{l-1}}]  
\]
Um uns an dieser Stelle die Notation zu vereinfachen führen wir an dieser Stelle für $ l=0,\dots,L $ die Zufallsvariable $ Y_l $ ein mit $ Y_0(\omega) \coloneqq Q_{h_0}(\omega) = J(\rho_{h_0}(\omega)) $ und $ Y_l \coloneqq Q_{h_l}(\omega) - Q_{h_{l-1}}(\omega) =  J(\rho_{h_l}(\omega)) - J(\rho_{h_{l-1}}(\omega))  $ für $ l=1,\dots,L $. Man beachte hierbei, dass wie bereits an früher Stelle erwähnt, beim Vergleich zweier verschiedener Level das gleiche Sample zugrunde gelegt wird. 
So folgt:
\[
	\mathbb{E}[Q_{h_L}] = \sum_{l=0}^L \mathbb{E}[Y_l]
\]
Dann ist der Multilevel Monte Carlo Schätzer gegeben durch die Summe der Monte Carlo Schätzer $ \widehat{Y}_{h,n_l}^{\text{MC}} $ für die einzelnen Erwartungswerte $ \mathbb{E}[Y_l] $:
\begin{align}
	\widehat{Q}_{\tilde{\mathcal{H}},\{ n_l \}_{l=0}^L }^{\text{MLMC}} = \sum_{l=0}^{L} \widehat{Y}_{h,n_l}^{\text{MC}} =  \sum_{l=0}^{L} \frac{1}{n_l} \sum_{i=1}^{n_l} \left( Q_{h_l}(\omega_i) - Q_{h_{l-1}}(\omega_i)\right)
\end{align}
Da für jedes $ l $ $ \widehat{Y}_{h,n_l}^{\text{MC}} $ getrennt berechnet und somit jeder Erwartungswert $ \mathbb{E}[Y_l] $ unabhängig geschätzt wird, erhalten wir die Varianz des MLMC-Schätzers durch $\mathbb{V}[ \widehat{Q}_{\tilde{\mathcal{H}},\{ n_l \}_{l=0}^L }^{\text{MLMC}} ] = \sum_{l=0}^{L} \frac{1}{n_l} \mathbb{V}[Y_l]$. Wie in \ref{badumts} lässt sich dann der mean square error dann ausdrücken durch:
\begin{align}
e(\widehat{Q}_{\tilde{\mathcal{H}},\{ n_l \}_{l=0}^L }^{\text{MLMC}})^2 &= \mathbb{E}  \left[ (\widehat{Q}_{\tilde{\mathcal{H}},\{ n_l \}_{l=0}^L }^{\text{MLMC}}- \mathbb{E}[Q])^2 \right] \nonumber \\
&= \mathbb{E}  \left[ (\widehat{Q}_{\tilde{\mathcal{H}},\{ n_l \}_{l=0}^L }^{\text{MLMC}} - \mathbb{E}[\widehat{Q}_{\tilde{\mathcal{H}},\{ n_l \}_{l=0}^L }^{\text{MLMC}}]+\mathbb{E}[\widehat{Q}_{\tilde{\mathcal{H}},\{ n_l \}_{l=0}^L }^{\text{MLMC}}]- \mathbb{E}[Q])^2 \right] \nonumber \\
&= \mathbb{E} \left[  (\widehat{Q}_{\tilde{\mathcal{H}},\{ n_l \}_{l=0}^L }^{\text{MLMC}}-\mathbb{E}[\widehat{Q}_{\tilde{\mathcal{H}},\{ n_l \}_{l=0}^L }^{\text{MLMC}}])^2 \right] +  \left(  \mathbb{E}[\widehat{Q}_{\tilde{\mathcal{H}},\{ n_l \}_{l=0}^L }^{\text{MLMC}}] - \mathbb{E}[Q] \right)^2 \nonumber \\
&= \underbrace{\sum_{l=0}^L \frac{1}{n_l} \mathbb{V}[Y_l]}_{(1)} + \underbrace{\left( \mathbb{E}[Q_{h_L}-Q] \right)^2}_{(2)}
\end{align}
\begin{Bemerkung}(Was lässt sich hieraus ablesen?)
	\begin{itemize}
		\item $ (2) $ stimmt mit $ (2) $ aus \ref{mseMC} überein. Insbesondere ist also wieder  $ h \lesssim \epsilon^{\frac{1}{\alpha}}  $ dafür hinreichend, dass der Diskretisierungs- und Lösungsfehler kleiner als $ \frac{\epsilon^2}{2} $  ausfällt.
		\item Um also einen kleineren RMSE als $ \epsilon $ zu erhalten, muss auch $ (1) \leq  \frac{\epsilon^2}{2} $ gelten.
		\item Unter Annahme \ref{Annahme1} gilt $ \mathbb{V}[Y_l] = \mathbb{V}[Q_{h_l}-Q_{h_{l-1}}] \stackrel{l \to \infty}{\to} 0  $, das bedeutet für uns: Je größer $ l $ und damit je feiner die Gitterweite $ h_l $ ist, desto weniger Samples werden benötigt um $ \mathbb{E}[Y_l] $ zu schätzen.
		\item Das gröbste
	\end{itemize}
\end{Bemerkung}



 \begin{algorithm}[H]
 	\DontPrintSemicolon
 	\SetAlgoLined
 	%\KwResult{}
 	\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
 	\Input{$ l_0,L_0 \in \N $ und $ N_0 = \{ n_{l_0},\dots,n_{L_0} \}$}
 	\Output{$\widehat{Q}_{\mathcal{H},\{ n_l \}_{l=l_0}^L }^{\text{MLMC}} $}
 	\BlankLine
 	
 	Setze $ \mathcal{R} = \{ l_0,\dots,L_0 \} $ und die Anzahl der benötigten Samples $ \Delta N = \{ \Delta n_l \}_{l=l_0}^{L_0} = N_0$ und $ i = 0 $\;
 	\While{$\Delta n_l > 0$ für mindestens ein $ l \in \mathcal{R} $ }{
 		\For{alle $ l \in \mathcal{R} $ mit $\Delta n_l > 0$ }{
 			Berechne Zielfunktional und benötigte Kosten: $ Y_l,C_l \leftarrow $ SubroutineEstimator$(\Delta n_l,l)$\;
 			Update $ C_l,\abs{\mathbb{E}[Y_l]},\mathbb{V}[Y_l] $ und setze $n_l = \Delta n_l, \Delta n_l=0$\;
 		}
 		Schätze die Exponenten $ \alpha,\beta,\gamma $ mit den Annahmen von Satz TODO\;
 		Schätze optimales $ N_i = \{ n_{l_i},\dots,n_{L_i} \} $ mit Formel TODO und berechne $\Delta N$\;
 		Teste auf schwache Konvergenz TODO\;
 		\eIf{nicht konvergiert}{
 			Setze $\mathcal{R} = \{ l_0,\dots,L_i,L_{i+1} \coloneqq L_i+1 \} $\;
 			i = i+1 \;
 			
 		} {
 		 $ \Delta N = \{0,\dots,0\}$\;
 		}
 	
 	}
 	\BlankLine
 	\KwResult{$\widehat{Q}_{\mathcal{H},\{ n_l \}_{l=l_0}^L }^{\text{MLMC}} $}
 	\caption{MLMC angewandt auf das Transportproblem}
 \end{algorithm}

\bigskip % add 12pt space in-between
 
 
 
%\begin{itemize}
%	\item Theorie Konvergenzannahmen
%	\item Resultate Giles + parrallelFEM Paper
%	\item Algorithmus
%\end{itemize}
%

