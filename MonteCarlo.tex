% !TeX root = bachelorarbeit.tex
%\cite{lapeyre2003introduction} \cite{sullivan2015introduction}
\subsection{Herleitung und Beispiel}
Wie in \cite{lapeyre2003introduction} wollen wir uns, um die Monte Carlo Methode von Grund auf einzuführen, zunächst mit der numerischen Integration beschäftigen. Grundsätzlich handelt es sich bei der Monte Carlo Methode um einen sogenannten Erwartungswertschätzer. Bevor wir also ein Problem mithilfe der Monte Carlo Methode lösen können, müssen wir die Größe, welche wir berechnen wollen, zunächst in der Form eines Erwartungswertes ausdrücken.
Wir suchen dann also einen Erwartungswert $ \mathbb{E}[X] $, wobei $ X $ eine Zufallsvariable, einen Zufallsvektor oder gar ein Zufallsfeld beschreiben kann.
Mithilfe der Monte-Carlo-Methode können wir dann versuchen eben diesen Erwartungswert zu schätzen. Dazu müssen wir $ X $ simulieren können. Damit ist gemeint, dass wir in der Lage sein müssen eine Realisierung $ (x_1,\dots,x_n) $ von $ (X_1,\dots,X_n) $ zu generieren (oft sagt man auch in Anlehnung an das Bernoulli'sche Urnenmodell 'zu ziehen'). Dabei sollen die Zufallsgrößen $ X_1,\dots,X_n $ unabhängig sein und die gleiche Verteilung besitzen wie die Zufallsgröße $ X $. Außerdem sei vorausgesetzt, dass der Erwartungswert $ \mathbb{E}[X] < \infty $ existiert.
Anschließend wird der gesuchte Erwartungswert durch
\[
	\mathbb{E}[X] \approx \frac{1}{n}(x_1 + \dots x_n)
\]
approximiert.

\begin{Beispiel}(Integral über $ [0,1]^d $ - aus \cite{lapeyre2003introduction})\\
	\label{BeispielIntegral}
	Angenommen, wir wollen für $ d \geq 1 $ folgendes Integral berechnen:
	\[
		I = \int_{[0,1]^d} f(u_1,\dots,u_d) \du_1\dots\du_d
	\]
	Wir können das Integral dann wie folgt als Erwartungswert ausdrücken: \\
	Sei $ X = f(U_1,\dots,U_d) $ ein 
	Zufallsvektor, wobei $ U_1,\dots,U_d $ unabhängig und auf $ [0,1] $ gleichverteilt sind, d.h. jedes $ U_i $ besitzt als Dichte $ f_i(x) = \mathds{1}_{[0,1]}(x) $.
	Dann ergibt sich so 
	\[
		I = \int_{[0,1]^d} f(u_1,\dots,u_d) \du_1\dots\du_d = \mathbb{E}[f(U_1,\dots,U_d)] = \mathbb{E}[X]
	\]
	Wir haben also das Integral, welches wir berechnen wollen, als Erwartungswert ausgedrückt. Nun müssen wir die Zufallsvariable $ X = f(U_1,\dots,U_d) $ simulieren.
	Dazu nehmen wir an, gleichverteilte Zufallsvariablen simulieren zu können. Die Simulation solcher Zufallsvariablen spielt in der numerischen Stochastik eine ganz besondere Rolle, denn oft werden andere Verteilungen durch Transformationen auf den Fall einer Gleichverteilung auf $ [0,1] $ reduziert.
	Sei also $ (U_i)_{i \geq 1} $ eine Folge unabhängiger Zufallsvariablen mit Gleichverteilung auf $ [0,1] $. Wir können dann mithilfe der simulierten Realisierungen $ (u_i)_{i \geq 1} $  von $ (U_i)_{i \geq 1} $ die Zufallsvariable $ X $ wie folgt definieren: Wir setzen
	\begin{align*}
		&X_1 = f(U_1,\dots,U_d), & &x_1 = f(u_1,\dots,u_d) \\
		&X_2 = f(U_{d+1},\dots,U_{2d}), & &x_2 = f(u_{d+1},\dots,u_{2d}) \\
		&X_i = f(U_{(i-1)d+1},\dots,U_{id}), & &x_2 = f(u_{(i-1)d+1},\dots,u_{id})
	\end{align*}
	Da $ (U_i)_{i \geq 1} $ eine Folge unabhängiger Zufallsvariablen ist, erhalten wir so unter der einzigen echten Voraussetzung, dass $ f $ messbar ist, nach dem Blockungslemma ebenfalls eine Folge unabhängiger Zufallsvariablen $ (X_i)_{i \geq 1} $.
	Außerdem erhalten wir so für ein großes $ n \in \N $ eine gute Approximation von $ I $ durch:
	\[
		I = \mathbb{E}[X] \approx \frac{1}{n}(x_1+\dots+x_n) = \frac{1}{n} (f(u_1,\dots,u_d)+\dots+f(u_{(n-1)d+1},\dots,u_{nd}))
	\]	
	Inbesondere haben wir keinerlei Regularität an $ f $ vorausgesetzt, es genügt bereits die bloße Messbarkeit von $ f $ .
\end{Beispiel}



 	Oft wollen wir über eine andere Grundmenge als $ [0,1]^d $ integrieren. 
 	Bei endlichen Mengen, etwa einer beschränkten Borelmenge $ B \subset \R^d $ mit $ 0 < \abs{B} \coloneqq \lambda^d(B) $ (hierbei ist $ \lambda^d(\cdot) $ das Borel-Lebesgue-Maß) lässt sich $ I = \int_{B} f(x) \dx $ ähnlich wie in obigem Beispiel berechnen.
	Für einen Zufallsvektor $ U $ mit Gleichverteilung $ U(B) $ auf $ B $ existiert nämlich der Erwartungswert $ f(U) $ und es gilt:
	\[
		\mathbb{E}[f(U)] = \int_{B} f(x) \frac{1}{\abs{B}} \dx = \frac{I}{\abs{B}}
	\]
	Wieder simulieren wir $ (U_i)_{i \geq 1} $ als Folge unabhängiger Zufallsvariablen mit identischer Verteilung zu $ U $. Dann erhalten wir:
	\[
		I = \abs{B} \cdot \mathbb{E}[f(U)] \approx \frac{\abs{B}}{n} \sum_{j=1}^{n}f(u_j)
	\]
	 Wollen wir hingegen ein Integral über $ \R^d $ auswerten, muss es uns in der Form 
	\[
	I = \int_{\R^d} g(x)f(x) \dx = \int_{\R^d} g(x_1,\dots,x_d)f(x_1,\dots,x_d) \dx
	\] 
	vorliegen. Dabei sei $ f(x) $ nichtnegativ und $ \int_{\R^d} f(x) \dx = 1 $.
	Dann lässt sich $ I $ schreiben als $ I = \mathbb{E}[g(X)] $ für eine Zufallsvariable $ X $ mit Werten in $ \R^d $ und Verteilung $ f(x) \dx $.
	Wir können also $ I $ approximieren durch
	\[
		I \approx \frac{1}{n}\sum_{i=1}^{n} g(x_i) \quad ,
	\]
	wobei $ (x_i)_{i \geq 1} $ Realisierungen der Zufallsvariablen $ (X_i)_{i \geq 1} $ sind, welche unabhängig und identisch zu $ X $ verteilt seien.
	
	Betrachten wir nun wieder die Monte Carlo Methode in einem etwas abstrakteren Sinne ganz allgemein.
	An der Stelle, an der wir letztlich die Realisierungen einer Zufallsvariable eingesetzt haben, also einen Erwartungswert durch $
	\mathbb{E}[X] \approx \frac{1}{n}(x_1 + \dots x_n)
	$ approximiert haben, haben wir stets gefordert, dass $ n $ groß ist. 
	Es stellt sich nun die Frage, wann $ n $ groß genug ist.
	Wir wollen uns deshalb noch abschließend damit beschäftigen, wann und wie die Methode konvergiert und was wir über die Genauigkeit der Approximation aussagen können.
	\subsection{Konvergenz und Genauigkeit}
	Damit die Methode überhaupt in irgendeiner Weise als nützlich zu erachten ist, bedarf es Möglichkeiten, den Fehler \[
	 \epsilon_n = \frac{1}{n}\sum_{i=1}^{n}X_i -  \mathbb{E}[X]
	\]
	abzuschätzen. Um diesem Problem beizukommen, bedienen wir uns zweier zentraler Aussagen der Wahrscheinlichkeitstheorie. Zum einen sagt uns das starke Gesetz großer Zahlen \ref{starkesGgZ}, dass unter der Voraussetzung $ \mathbb{E}[\abs{X}]<\infty $ der Fehler $ \epsilon_n $ für $ n \to \infty $ für fast alle $ \omega \in \Omega $ gegen $ 0 $ konvergiert. Wir erhalten also zunächst Konvergenz der Methode in einem sehr grundlegenden Sinn. Aus dem zentralen Grenzwertsatz \ref{ZGWS} lassen sich zum anderen Aussagen über die Genauigkeit der Methode und letztlich somit auch der Art der Konvergenz ableiten. Nach \ref{ZGWS} erhalten wir nämlich für eine u.i.v.-Folge $ (X_i)_{i \in \N} $ mit gleicher Verteilung wie $ X $ und $ \mathbb{E}[X^2] < \infty $, dass
	 \[ 
	 \frac{\sqrt{n}}{\sqrt{\mathbb{V}[X]}} \epsilon_n =  \frac{\frac{1}{\sqrt{n}}\sum_{i=1}^{n}X_i-\sqrt{n}\mathbb{E}[X]}{\sqrt{\mathbb{V}[X]}} = \frac{\sum_{i=1}^{n}X_i-n\mathbb{E}[X]}{\sqrt{n\mathbb{V}[X]}} \eqqcolon \hat{S}_n \stackrel{\mathcal{D}}{\to}  \widetilde{\mathcal{N}} \text{ für } n \to \infty \ ,
	 \]
	 wobei  $\widetilde{\mathcal{N}}$ eine standardnormalverteilte Zufallsvariable ist. Die Wurzel der Varianz wird im Folgenden noch des Öfteren auftauchen, weswegen wir an dieser Stelle die sogenannte Standardabweichung $ \sigma \coloneqq \sqrt{\mathbb{V}[X]} $ einführen.
	 Da also 
	 \[ \lim\limits_{n \to \infty} \mathbb{P}^{\hat{S}_n}((-\infty,x]) = \mathbb{P}^{ \widetilde{\mathcal{N}}}((-\infty,x])
	 \]
	 gilt, ist insbesondere für $ a \leq b $
	 \[
	 	\lim\limits_{n \to \infty} \mathbb{P}(\frac{\sigma}{\sqrt{n}}a \leq \epsilon_n \leq \frac{\sigma}{\sqrt{n}}b) = \lim\limits_{n \to \infty} \mathbb{P}(a \leq \hat{S}_n \leq b) = \int\limits_a^b \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}} \dx \ .
	 \]
	 An dieser Stellen wollen wir kurz innehalten und uns überlegen, was obiges Resultat für den Fehler der Monte Carlo Methode denn praktisch gesehen bedeutet. 
	 \begin{itemize}
	 	\item Der zentrale Grenzwertsatz liefert uns kein zu der Folgerung aus dem starken Gesetz großer Zahlen vergleichbares Resultat, denn es ist  $ \lim_{n \to \infty} \mathbb{P}(\epsilon_n = 0) = 0 $ nach obiger Überlegung.
	 	\item Der zentrale Grenzwertsatz erlaubt uns ebenso \underline{nicht} eine für andere Verfahren typische Fehlerschranke der Form $ \epsilon_n \leq M_n $ für eine von n und möglicherweise anderen Faktoren, wie z.B. Ausgangsdaten, abhängigen Schranke $ M_n $ aufzustellen. Grund dafür ist, dass der Träger von $\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}$ ganz $ \R $ ist.
	 	\item Was der zentrale Grenzwertsatz uns jedoch erlaubt, ist, ein sogenanntes $ 95\% $ Konfidenzintervall für $ \epsilon_n $ zu bestimmen. Das bedeutet, dass das tatsächliche Ergebnis mit einer Wahrscheinlichkeit von mindestens $ 95 \% $ im gegebenen Intervall enthalten ist. Denn, da 
	 	\[
	 		\mathbb{P}(\abs{N} \leq 1.96) \approx 0.95 \quad ,
	 	\]
	 	können wir wegen
	 	\[
	 		\lim\limits_{n\to\infty}\mathbb{P}(-1.96\frac{\sigma}{\sqrt{n}}\leq \epsilon_n \leq 1.96\frac{\sigma}{\sqrt{n}}) \approx 0.95 \quad (\star)
	 	\]
	 	ein Konfidenzintervall für $ \mathbb{E}[X] $ der Form
	 	\[
	 		[\hat{\mu}-1.96\frac{\sigma}{\sqrt{n}},\hat{\mu}+1.96\frac{\sigma}{\sqrt{n}}] \quad \text{ für ein } \hat{\mu} \in \R  
	 	\]
	 	angeben. In der Praxis nehmen wir näherungsweise an, dass $ (\star) $ auch für ein festes $ n \in \N $ erfüllt ist, und entledigen uns so des Grenzwertes. Somit wird dann insbesondere die Wahl $ \hat{\mu} = \frac{1}{n}\sum_{i=1}^{n}(x_1,\dots,x_n) $ gerechtfertigt.
	 \end{itemize}
 	Wir erhalten also (unter den eben erklärten Annahmen) eine Konvergenzrate des (wahrscheinlichen) Fehlers von $ \frac{\sigma}{\sqrt{n}} $. Dieses Resultat mag auf den ersten Blick relativ ernüchternd wirken, allerdings existieren Fälle, in denen solch eine langsame Methode die bestmögliche ist. \cite{lapeyre2003introduction} nennt hierzu zum Beispiel Integrale in mehr als $ 100 $ Dimensionen oder besonders schwere parabolische Differentialgleichungen. Denn anders als andere Verfahren, besonders deutlich wird dies erneut auf der Ebene der Quadratur (vgl Beispiel \ref{mlmcbeispiel}), sind Monte Carlo Methoden nicht vom sogenannten 'Curse of dimensionality' betroffen. Während bei anderen Quadraturformeln die Anzahl der benötigten Quadraturpunkte mit der Dimension im Exponent steigt, gelten obige Resultate unabhängig von der Dimension. Wir werden später Zufallsprobleme mit sehr hohen Dimensionen betrachten, da wir in einer Bodenschicht jede Zelle als einzelne Zufallsvariable betrachten werden. Deswegen ziehen wir die Monte Carlo Methode bzw. später die Multilevel Monte Carlo Methode einem anderen Ansatz zum Lösen stochastischer partieller Differentialgleichungen, wie etwa stochastische Finite Elemente, vor.
 	Außerdem lohnt es sich zu erwähnen, dass wir im Falle der numerischen Integration - bis auf Integrierbarkeit und Messbarkeit - keine Voraussetzungen an die Regularität der Funktion $ f $ gestellt haben.\\
 	Obiges Resultat legt außerdem nahe, dass es entscheidend für eine Aussage über die Konvergenz und Güte der Methode ist, die Standardabweichung $ \sigma $ zu kennen, oder zumindest über einen guten Schätzer für $ \sigma $ zu verfügen.
 	Falls uns $ \sigma $ bzw. $ \mathbb{V} $ nämlich sogar exakt bekannt ist, können wir die sogenannte Chebyshev Ungleichung \ref{ChebCheb} ausnutzen:
 	Da $ (X_i)_{i \in \N} $ eine u.i.v.-Folge mit Verteilung wie $ X $ ist, gilt nämlich mit den üblichen Rechenregeln für die Varianz (zu finden z.B. in \cite{brokate2016grundwissen} auf den Seiten 778 und 779)
 	\[
 		\mathbb{V}[\frac{1}{n}\sum_{i=1}^{n}X_i] =  \frac{1}{n^2} \sum_{i=1}^{n} \mathbb{V}[X] = \frac{\mathbb{V}[X]}{n}
 	\]
 	Dann besagt die Chebychev Ungleichung für alle $ t \geq 0 $:
 	\[
 		\mathbb{P}\left(\abs{\frac{1}{n}\sum_{i=1}^{n}X_i-\mathbb{E}[X]} \geq t \right) \leq \frac{\mathbb{V}[X]}{nt^2}
 	\]
 	Für uns bedeutet das insbesondere, dass für jedes $ \epsilon \in (0,1] $  die berechnete Monte-Carlo Approximation $ \frac{1}{n}\sum_{i=1}^{N} $ mit einer Wahrscheinlichkeit von $ 1-\epsilon $ weniger als $ \left( \frac{\mathbb{V}[X]}{n\epsilon}\right)^{\frac{1}{2}} $ von dem tatsächlichen Erwartungswert $ \mathbb{E}[X] $ entfernt ist.
 	In der Literatur (z.B. in \cite{sullivan2015introduction}) finden sich einige Weiterentwicklungen der Monte Carlo Methode. Abgesehen von der Multilevel Monte Carlo Methode, welche wir in Abschnitt \ref{MLMC} behandeln werden, wollen wir uns hier auf die oben erklärte Standard-Variante beschränken.
 	
 
 	
	
	
	


